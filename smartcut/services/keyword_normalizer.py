"""
keyword_normalizer.py â€” version avec mode configurable
------------------------------------------------------
Normalisation des mots-clÃ©s avec mapping manuel + embeddings + modes de filtrage.
"""

import json
from pathlib import Path

from sentence_transformers import SentenceTransformer, util

from shared.utils.config import KW_CACHE_FILE_SC, KW_FORBIDDEN_FILE_SC, KW_MAPPING_FILE_SC
from shared.utils.logger import LoggerProtocol, ensure_logger, with_child_logger
from shared.utils.settings import get_settings

settings = get_settings()
MODEL_NAME = settings.keyword_normalizer.model_name_key
MODE = settings.keyword_normalizer.mode
SIMILARITY_THRESHOLD = settings.keyword_normalizer.similarity_threshold


class KeywordNormalizer:
    """
    Normalise les mots-clÃ©s via un mapping manuel, embeddings et fichiers JSON.
    """

    @with_child_logger
    def __init__(
        self,
        model_name: str = MODEL_NAME,
        threshold: float = SIMILARITY_THRESHOLD,
        mode: str = MODE,  # "full", "strict" ou "mixed"
        mapping_path: Path = KW_MAPPING_FILE_SC,
        forbidden_path: Path = KW_FORBIDDEN_FILE_SC,
        logger: LoggerProtocol | None = None,
    ) -> None:
        logger = ensure_logger(logger, __name__)
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
        self.mapping = self._load_mapping(mapping_path, logger)
        self.forbidden = self._load_forbidden(forbidden_path, logger)
        self.cache = self._load_cache(logger)
        self.mode = mode
        # Tri et sauvegarde automatique des fichiers au dÃ©marrage
        self._save_mapping(logger)
        self._save_forbidden(logger)

        if self.mode not in {"full", "strict", "mixed"}:
            raise ValueError("Mode invalide : choisissez 'full', 'strict' ou 'mixed'.")

    # -------------------- Gestion fichiers -------------------- #
    @with_child_logger
    def _load_mapping(self, path: Path, logger: LoggerProtocol | None = None) -> dict[str, str]:
        """
        Charge le mapping depuis mapping.json, ou renvoie un dict vide si absent.
        """
        logger = ensure_logger(logger, __name__)
        if path.exists():
            try:
                with open(path, encoding="utf-8") as f:
                    data = json.load(f)

                if not isinstance(data, dict):
                    logger.warning("âš ï¸ Le fichier mapping.json n'est pas un dictionnaire valide.")
                    return {}

                sorted_data = dict(sorted(data.items()))
                logger.info("âœ… Mapping chargÃ© depuis %s (%d entrÃ©es).", path, len(sorted_data))
                return {str(k).lower(): str(v).lower() for k, v in sorted_data.items()}

            except json.JSONDecodeError:
                logger.error("âŒ Erreur de parsing JSON pour %s â€” fichier ignorÃ©.", path)
                return {}
            except Exception as exc:
                logger.error("ðŸ’¥ Erreur inattendue lors du chargement du mapping : %s", exc)
                return {}
        else:
            logger.info("â„¹ï¸ Aucun mapping.json trouvÃ© â€” aucun mapping appliquÃ©.")
            return {}

    @with_child_logger
    def _save_mapping(self, logger: LoggerProtocol | None = None) -> None:
        """
        Sauvegarde le mapping triÃ© alphabÃ©tiquement dans mapping.json.
        """
        logger = ensure_logger(logger, __name__)
        try:
            sorted_mapping = dict(sorted(self.mapping.items()))
            with open(KW_MAPPING_FILE_SC, "w", encoding="utf-8") as f:
                json.dump(sorted_mapping, f, indent=2, ensure_ascii=False)
            logger.info("Mapping sauvegardÃ© (%d entrÃ©es, triÃ© alphabÃ©tiquement).", len(sorted_mapping))
        except Exception as e:
            logger.error("Erreur lors de la sauvegarde du mapping : %s", e)

    @with_child_logger
    def _load_forbidden(self, path: Path, logger: LoggerProtocol | None = None) -> set[str]:
        """
        Charge la liste de mots interdits depuis forbidden.json et la trie alphabÃ©tiquement.
        """
        logger = ensure_logger(logger, __name__)
        if path.exists():
            try:
                with open(path, encoding="utf-8") as f:
                    data = json.load(f)
                sorted_data = sorted({str(d).lower() for d in data})
                logger.info("Liste de mots interdits chargÃ©e depuis %s (%d entrÃ©es triÃ©es).", path, len(sorted_data))
                return set(sorted_data)
            except json.JSONDecodeError:
                logger.warning("Erreur de parsing JSON pour %s â€” pas de filtre appliquÃ©.", path)
        else:
            logger.warning("Aucun forbidden.json trouvÃ© â€” pas de filtre appliquÃ©.")
        return set()

    @with_child_logger
    def _save_forbidden(self, logger: LoggerProtocol | None = None) -> None:
        """
        Sauvegarde la liste de mots interdits triÃ©e alphabÃ©tiquement dans forbidden.json.
        """
        logger = ensure_logger(logger, __name__)
        try:
            sorted_forbidden = sorted(set(self.forbidden))
            with open(KW_FORBIDDEN_FILE_SC, "w", encoding="utf-8") as f:
                json.dump(sorted_forbidden, f, indent=2, ensure_ascii=False)
            logger.info("Liste de mots interdits sauvegardÃ©e (%d entrÃ©es triÃ©es).", len(sorted_forbidden))
        except Exception as e:
            logger.error("Erreur lors de la sauvegarde de la liste interdite : %s", e)

    @with_child_logger
    def _load_cache(self, logger: LoggerProtocol | None = None) -> dict[str, str]:
        """
        Charge le cache depuis keyword_cache.json et le trie alphabÃ©tiquement.
        """
        logger = ensure_logger(logger, __name__)
        if KW_CACHE_FILE_SC.exists():
            try:
                with open(KW_CACHE_FILE_SC, encoding="utf-8") as f:
                    cache = json.load(f)
                sorted_cache = dict(sorted(cache.items()))
                logger.info("Cache chargÃ© depuis %s (%d entrÃ©es triÃ©es).", KW_CACHE_FILE_SC, len(sorted_cache))
                return sorted_cache
            except json.JSONDecodeError:
                logger.warning("Cache corrompu, recrÃ©ation.")
        else:
            logger.info("Aucun cache existant trouvÃ© â€” initialisation dâ€™un cache vide.")
        return {}

    @with_child_logger
    def _save_cache(self, logger: LoggerProtocol | None = None) -> None:
        """
        Sauvegarde le cache triÃ© alphabÃ©tiquement dans keyword_cache.json.
        """
        logger = ensure_logger(logger, __name__)
        try:
            sorted_cache = dict(sorted(self.cache.items()))
            with open(KW_CACHE_FILE_SC, "w", encoding="utf-8") as f:
                json.dump(sorted_cache, f, indent=2, ensure_ascii=False)
            logger.info("Cache sauvegardÃ© (%d entrÃ©es, triÃ© alphabÃ©tiquement).", len(sorted_cache))
        except Exception as e:
            logger.error("Erreur lors de la sauvegarde du cache : %s", e)

    # -------------------- Normalisation -------------------- #

    @with_child_logger
    def _normalize_with_embeddings(self, word: str, candidates: list[str], logger: LoggerProtocol | None = None) -> str:
        """
        Compare un mot avec les candidats via embeddings et retourne le plus proche.
        """
        logger = ensure_logger(logger, __name__)
        word_emb = self.model.encode(word, convert_to_tensor=True)
        if len(word_emb.shape) > 1 and word_emb.shape[0] > 1:
            logger.debug(f"âš™ï¸ Moyenne des {word_emb.shape[0]} tokens pour '{word}'.")
            word_emb = word_emb.mean(dim=0, keepdim=True)
        logger.debug("Nombre de candidats reÃ§us : %d", len(candidates))
        logger.debug("Exemples de candidats : %s", candidates[:10])
        cand_emb = self.model.encode(candidates, convert_to_tensor=True)

        # ðŸ§  --- LOGS DE DIAGNOSTIC ---
        logger.debug(
            "ðŸ” _normalize_with_embeddings â€” word='%s' | word_emb.shape=%s | cand_emb.shape=%s",
            word,
            tuple(word_emb.shape),
            tuple(cand_emb.shape),
        )
        # âš™ï¸ --- PATCH : moyenne des tokens si le mot d'entrÃ©e produit plusieurs embeddings ---
        if len(word_emb.shape) > 1 and word_emb.shape[0] > 1:
            logger.debug(
                "âš™ï¸ Moyenne des %d tokens du mot '%s' pour un embedding global.",
                word_emb.shape[0],
                word,
            )
            word_emb = word_emb.mean(dim=0, keepdim=True)

        scores = util.cos_sim(word_emb, cand_emb)[0]

        # Log les stats du tensor
        logger.debug(
            "Scores tensor shape=%s | device=%s | valeurs=%s",
            tuple(scores.shape),
            scores.device,
            scores[:10],  # affiche les 10 premiers max
        )

        best_idx = int(scores.argmax())
        best_score = float(scores[best_idx])
        logger.debug(
            "âœ… Best match index=%d | best_score=%.4f | candidate='%s'",
            best_idx,
            best_score,
            candidates[best_idx] if candidates else "N/A",
        )

        return candidates[best_idx] if best_score >= self.threshold else word

    @with_child_logger
    def normalize(self, word: str, logger: LoggerProtocol | None = None) -> str:
        logger = ensure_logger(logger, __name__)
        word = word.lower().strip()

        if word in self.cache:
            return self.cache[word]

        if word in self.mapping:
            norm = self.mapping[word]
            self.cache[word] = norm
            return norm

        candidates = list(set(self.mapping.values()))
        norm = self._normalize_with_embeddings(word, candidates, logger)
        logger.debug(f"norm = {norm}")
        self.cache[word] = norm
        self._save_cache()

        if norm != word:
            logger.info("â†’ '%s' reconnu comme '%s' (similaritÃ© sÃ©mantique)", word, norm)
        return norm

    # -------------------- Mode de traitement -------------------- #

    @with_child_logger
    def normalize_keywords(self, keywords: str | list[str], logger: LoggerProtocol | None = None) -> list[str]:
        """
        Normalise une liste ou une chaÃ®ne de mots-clÃ©s.
        """
        logger = ensure_logger(logger, __name__)
        # ðŸ”¹ Accepte str ou list
        if isinstance(keywords, str):
            mots = [m.strip() for m in keywords.split(",") if m.strip()]
        elif isinstance(keywords, list):
            mots = [m.strip() for m in keywords if isinstance(m, str) and m.strip()]
        else:
            mots = []

        # ðŸ”¹ Normalisation individuelle
        normalises = [self.normalize(m) for m in mots]

        # ðŸ”¹ Gestion du mode
        if self.mode == "strict":
            normalises = [m for m in normalises if m in self.mapping.values()]
        elif self.mode == "mixed":
            normalises = sorted(set(normalises), key=lambda x: x not in self.mapping.values())
        else:
            normalises = sorted(set(normalises))

        # ðŸ”¹ Filtrage final
        normalises = [m for m in normalises if m.lower() not in self.forbidden]

        return normalises
