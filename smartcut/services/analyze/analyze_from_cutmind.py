from __future__ import annotations

from collections import Counter
from math import ceil
from pathlib import Path
import shutil

from transformers import PreTrainedModel, ProcessorMixin

from cutmind.db.repository import CutMindRepository
from cutmind.models_cm.db_models import Segment
from shared.executors.ffmpeg_utils import get_duration
from shared.models.exceptions import CutMindError, ErrCode, get_step_ctx
from shared.models.timer_manager import Timer
from shared.status_orchestrator.statuses import OrchestratorStatus
from shared.utils.config import BATCH_FRAMES_DIR_SC
from shared.utils.logger import LoggerProtocol, ensure_logger, with_child_logger
from shared.utils.safe_segments import safe_segments
from smartcut.executors.analyze.analyze_batches import process_batches
from smartcut.executors.analyze.analyze_torch_utils import (
    estimate_visual_tokens,
    release_gpu_memory,
    vram_gpu,
)
from smartcut.executors.analyze.analyze_utils import (
    delete_frames,
)
from smartcut.executors.analyze.extract_frames import extract_segment_frames
from smartcut.executors.analyze.prep_analyze import cleanup_temp, open_vid, release_cap
from smartcut.executors.ia.load_model import load_and_batches
from smartcut.models_sc.ai_result import AIOutputType, AIResult
from smartcut.models_sc.ia_models import AIContext

KeywordsBatches = list[AIResult]


# ===========================================================
# ðŸ§  FONCTION PRINCIPALE : analyse de la vidÃ©o par segments
# ===========================================================
@safe_segments
@with_child_logger
def analyze_from_cutmind(
    seg: Segment,
    frames_per_segment: int = 3,
    auto_frames: bool = True,
    base_rate: int = 5,
    fps_extract: float = 1.0,
    prompt_name: str = "keywords",
    system_prompt: str = "system_keywords",
    force: bool = False,
    logger: LoggerProtocol | None = None,
) -> tuple[str, list[str]]:
    """
    Extrait des frames pour chaque segment et gÃ©nÃ¨re les mots-clÃ©s IA.
    Retourne un mapping {segment_uid: keywords}.
    """
    logger = ensure_logger(logger, __name__)
    repo = CutMindRepository()
    try:
        # Nettoyage rÃ©pertoires temporaires
        cleanup_temp()

        if not seg.description:
            seg.description = ""
        if not seg.output_path:
            raise CutMindError(
                "âŒ Erreur segment : Aucune output path dÃ©fini.",
                code=ErrCode.DB,
                ctx=get_step_ctx({"seg.id": seg.id}),
            )
        cap, video_name = open_vid(seg.output_path)

        # Chargement du modÃ¨le IA + paramÃ¨tres de batching
        free_gb, total_gb = vram_gpu()
        logger.info(f"ðŸ“Š VRAM avant chargement : {free_gb:.2f} Go / {total_gb:.2f} Go")
        processor, model, model_name, batch_size, model_precision = load_and_batches(free_gb=free_gb)
        free_vram_gb, total_vram_gb = vram_gpu()
        logger.info(
            f"ðŸ§® VRAM libre : {free_vram_gb:.2f} Go / {total_vram_gb / 1e9:.2f} Go total | "
            f"PrÃ©cision : {model_precision} | ModÃ¨le : {model_name} | "
            f"â†’ Batch recommandÃ© = {batch_size}"
        )

        start: float = 0.0
        if seg.duration is None:
            end: float = get_duration(Path(seg.output_path))
        else:
            end = seg.duration

        logger.info(f"ðŸŽ¬ Analyse segment {seg.id} ({start:.2f}s â†’ {end:.2f}s)")

        frame_paths = extract_segment_frames(cap, video_name, start, end, auto_frames, fps_extract, base_rate)
        if not frame_paths:
            logger.warning(f"Aucune frame extraite pour le segment {seg.id}")
            raise CutMindError(
                "âŒ Erreur segment : Aucune frame extraite pour le segment.",
                code=ErrCode.VIDEO,
                ctx=get_step_ctx({"video_name": video_name, "seg.id": seg.id}),
            )

        if force or is_empty(seg.description) or is_empty(seg.category):
            output_type: AIOutputType = "full"
        else:
            output_type = "keywords"

        logger.info(f"ðŸŽ¬ Analyse segment {seg.id} ({start:.2f}s â†’ {end:.2f}s) : {len(frame_paths)} frames extraites.")
        with Timer(f"Traitement Keywords : {seg.id}", logger):
            context = run_ai_pipeline_v25(
                video_name=video_name,
                start=start,
                end=end,
                frame_paths=frame_paths,
                batch_size=batch_size,
                processor=processor,
                model=model,
                output_type=output_type,
                logger=logger,
            )

        # --- DESCRIPTION ---
        if force:
            seg.description = context.description or ""
        else:
            if is_empty(seg.description) and context.description:
                seg.description = context.description or ""

        # --- CATEGORY ---
        if force:
            seg.category = context.category
        else:
            if is_empty(seg.category) and context.category:
                seg.category = context.category
        if context.keywords:
            seg.keywords = context.keywords or []
            logger.debug(f"ðŸ§  Segment {seg.id} keywords: {seg.keywords}")
        logger.debug(f"ðŸ§  Segment {seg.id} description: {seg.description}")

        # --- ðŸ’¾ Mise Ã  jour du segment
        logger.debug(f"ðŸ” seg.id={seg.id} mem_id={id(seg)}")

        seg.ai_model = model_name
        seg.status = OrchestratorStatus.IA_DONE
        repo.update_segment_validation(seg)
        if not seg.id:
            raise CutMindError(
                "âŒ Erreur DB : aucun seg.id.",
                code=ErrCode.DB,
                ctx=get_step_ctx({"video_name": video_name}),
            )
        if seg.keywords:
            # normalizer = KeywordNormalizer()
            # seg.keywords = normalizer.normalize_keywords(seg.keywords)
            repo.insert_keywords_standalone(segment_id=seg.id, keywords=seg.keywords)

        logger.debug(f"ðŸ’¾ Session mise Ã  jour (segment {seg.id})")
        # logger.debug(f"session : {session}")

        release_cap(cap)
        release_gpu_memory(model)
        free, total = vram_gpu()
        logger.info(f"ðŸ§¹ VRAM nettoyÃ©e ('full release') â†’ VRAM libre : {free / 1e9:.2f} Go / {total / 1e9:.2f} Go")
        logger.info("âœ… Analyse complÃ¨te terminÃ©e.")
        return seg.description, seg.keywords
    except CutMindError as err:
        raise err.with_context(get_step_ctx({"segment_id": seg.id})) from err
    except Exception as exc:
        raise CutMindError(
            "Erreur lors du traitement IA.",
            code=ErrCode.UNEXPECTED,
            ctx=get_step_ctx({"segment_id": seg.id}),
        ) from exc


def run_prompt_on_batches(
    *,
    video_name: str,
    start: float,
    end: float,
    frame_paths: list[str],
    batch_size: int,
    processor: ProcessorMixin,
    model: PreTrainedModel,
    prompt_name: str,
    system_prompt: str,
    logger: LoggerProtocol,
) -> KeywordsBatches:
    all_batches: KeywordsBatches = []

    num_batches = ceil(len(frame_paths) / batch_size)

    for b in range(num_batches):
        batch_paths = frame_paths[b * batch_size : (b + 1) * batch_size]
        if not batch_paths:
            continue

        batch_dir = (
            BATCH_FRAMES_DIR_SC / f"{video_name}_seg_{int(start * 10)}_{int(end * 10)}" / prompt_name / f"b{b + 1}"
        )
        batch_dir.mkdir(parents=True, exist_ok=True)

        for src_path in batch_paths:
            dst_path = batch_dir / Path(src_path).name
            try:
                shutil.copy(src_path, dst_path)
            except Exception as exc:
                raise CutMindError(
                    "âŒ Erreur de copie des frames.",
                    code=ErrCode.UNEXPECTED,
                    ctx=get_step_ctx(
                        {
                            "video_name": video_name,
                            "src_path": src_path,
                            "dst_path": dst_path,
                        }
                    ),
                ) from exc

        tokens, limit = estimate_visual_tokens(len(batch_paths))
        logger.info(
            "ðŸ“¦ Batch %d/%d | Frames=%d | Tokens=%s/%s",
            b + 1,
            num_batches,
            len(batch_paths),
            f"{tokens:,}",
            f"{limit:,}",
        )

        batch_result = process_batches(
            video_name=video_name,
            start=start,
            end=end,
            frame_paths=frame_paths,
            batch_size=batch_size,
            batch_dir=batch_dir,
            batch_paths=batch_paths,
            processor=processor,
            model=model,
            prompt_name=prompt_name,
            system_prompt=system_prompt,
        )

        all_batches.append(batch_result)

        release_gpu_memory(model, cache_only=True)
        free, total = vram_gpu()
        logger.debug(f"ðŸ§¹ all_batches : {all_batches}")
        logger.info(
            "ðŸ§¹ VRAM nettoyÃ©e ('cache_only') â†’ VRAM libre : %.2f Go / %.2f Go",
            free / 1e9,
            total / 1e9,
        )
        delete_frames(batch_dir)

    return all_batches


def merge_description(batches: list[AIResult]) -> str | None:
    descriptions = [batch["description"] for batch in batches if batch.get("description")]
    return descriptions[-1] if descriptions else None


def merge_category(batches: list[AIResult]) -> str | None:
    categories: list[str] = []

    for batch in batches:
        keywords = batch.get("keywords")
        if keywords:
            categories.append(keywords[0])

    if not categories:
        return None

    return Counter(categories).most_common(1)[0][0]


def merge_keywords(batches: list[AIResult]) -> list[str]:
    keywords: set[str] = set()

    for batch in batches:
        batch_keywords = batch.get("keywords")
        if batch_keywords:
            keywords.update(batch_keywords)

    return sorted(keywords)


def run_ai_pipeline_v25(
    *,
    video_name: str,
    start: float,
    end: float,
    frame_paths: list[str],
    batch_size: int,
    processor: ProcessorMixin,
    model: PreTrainedModel,
    output_type: AIOutputType = "full",
    logger: LoggerProtocol,
) -> AIContext:
    context = AIContext()

    # ===== PASSAGE 1 : description + catÃ©gorie =====
    logger.info("ðŸ§  IA Pass 1 â€” Description + CatÃ©gorie")

    pass1_batches = run_prompt_on_batches(
        video_name=video_name,
        start=start,
        end=end,
        frame_paths=frame_paths,
        batch_size=batch_size,
        processor=processor,
        model=model,
        prompt_name="keywords",
        system_prompt="system_keywords",
        logger=logger,
    )

    for i, batch in enumerate(pass1_batches):
        logger.debug(
            "ðŸ§ª Batch %d â†’ description=%s | keywords=%s",
            i,
            batch.get("description"),
            batch.get("keywords"),
        )
    context.description = merge_description(pass1_batches) or ""
    context.category = merge_category(pass1_batches)

    logger.info(
        "ðŸ§  Pass 1 result â†’ category=%s",
        context.category,
    )

    if not context.category:
        logger.warning("âš ï¸ Aucune catÃ©gorie dÃ©tectÃ©e, on arrÃªte le pipeline IA ici.")

    else:
        # ===== PASSAGE 2 : keywords guidÃ©s =====
        logger.info("ðŸ§  IA Pass 2 â€” Keywords guidÃ©s")

        pass2_batches = run_prompt_on_batches(
            video_name=video_name,
            start=start,
            end=end,
            frame_paths=frame_paths,
            batch_size=batch_size,
            processor=processor,
            model=model,
            prompt_name=context.category,
            system_prompt="system_keywords",
            logger=logger,
        )

        context.keywords = merge_keywords(pass2_batches)

    return context


def is_empty(value: str | None) -> bool:
    return value is None or not value.strip()
